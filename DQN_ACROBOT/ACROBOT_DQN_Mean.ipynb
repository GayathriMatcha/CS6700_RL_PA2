{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b2304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a16f6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA=0.99\n",
    "BATCH_SIZE=512\n",
    "BUFFER_SIZE=50000\n",
    "MIN_REPLAY_SIZE=10000\n",
    "EPSILON_START=1.0\n",
    "EPSILON_END=0.02\n",
    "EPSILON_DECAY=150000\n",
    "MAX_REWARD_REACH = -100\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "LR =1e-5\n",
    "ENV_Name='Acrobot-v1'\n",
    "TOTAl_episode =250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e106af7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imaging/anaconda3/lib/python3.11/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/imaging/anaconda3/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/imaging/anaconda3/lib/python3.11/site-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(ENV_Name)\n",
    "env.seed(0)\n",
    "env.env.book_or_nips = 'nips'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6051c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN_Mean(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(DuelingDQN_Mean, self).__init__()\n",
    "\n",
    "        in_features = int(np.prod(env.observation_space.shape))\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        '''Value stream'''\n",
    "        self.value_stream = nn.Linear(128, 1)\n",
    "\n",
    "        '''Advantage stream'''\n",
    "        self.advantage_stream = nn.Linear(128, env.action_space.n)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "\n",
    "        '''Combining value and advantage streams'''\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        # q_values = value + (advantage - advantage.max(dim=1, keepdim=True)[0])\n",
    "\n",
    "        return q_values\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self(obs_t.unsqueeze(0))\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.cpu().detach().item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6dc38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN_Max(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(DuelingDQN_Max, self).__init__()\n",
    "\n",
    "        in_features = int(np.prod(env.observation_space.shape))\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        '''Value stream'''\n",
    "        self.value_stream = nn.Linear(128, 1)\n",
    "\n",
    "        '''Advantage stream'''\n",
    "        self.advantage_stream = nn.Linear(128, env.action_space.n)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "\n",
    "        '''Combining value and advantage streams'''\n",
    "#         q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        q_values = value + (advantage - advantage.max(dim=1, keepdim=True)[0])\n",
    "\n",
    "        return q_values\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self(obs_t.unsqueeze(0))\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.cpu().detach().item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e436779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iteration_no,Mean_Flag = True):\n",
    "    Reward_max = -1000\n",
    "    replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "    rew_buffer = deque([0.0], maxlen=100)\n",
    "\n",
    "    episode_reward = 0\n",
    "    \n",
    "    if Mean_Flag:\n",
    "        online_net = DuelingDQN_Mean(env).to(device)\n",
    "        target_net = DuelingDQN_Mean(env).to(device)\n",
    "        \n",
    "    else :\n",
    "        online_net = DuelingDQN_Max(env).to(device)\n",
    "        target_net = DuelingDQN_Max(env).to(device)\n",
    "\n",
    "    target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "    optimizer = torch.optim.Adam(online_net.parameters(), lr=LR)\n",
    "\n",
    "    '''Initialize replay buffer with MIN_REPLAY_SIZE in deque '''\n",
    "    obs = env.reset()\n",
    "    for _ in range(MIN_REPLAY_SIZE):\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        new_obs, rew, done, _ = env.step(action)\n",
    "        transition = (obs, action, rew, done, new_obs)\n",
    "        replay_buffer.append(transition)\n",
    "        obs = new_obs\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "\n",
    "\n",
    "    # Main Training Loop\n",
    "    Average_reward = []\n",
    "    obs = env.reset()\n",
    "#     for step in itertools.count():\n",
    "    for step in range(TOTAl_episode):\n",
    "        epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])\n",
    "\n",
    "        rnd_sample = random.random()\n",
    "        if rnd_sample <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            obs = torch.as_tensor(obs, dtype=torch.float32).to(device)\n",
    "            action = online_net.act(obs)\n",
    "            obs = obs.cpu().numpy()\n",
    "\n",
    "        new_obs, rew, done, _ = env.step(action)\n",
    "        transition = (obs, action, rew, done, new_obs)\n",
    "        replay_buffer.append(transition)\n",
    "        obs = new_obs\n",
    "\n",
    "        episode_reward += rew\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            rew_buffer.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "\n",
    "        # # If we solved it lets just watch it play, put in last\n",
    "        if step % 10000 == 0:\n",
    "            if step !=0:\n",
    "                print(f'current_Reward_max {Reward_max}')\n",
    "                print(f'Trained_Reward_max {np.mean(rew_buffer)}')\n",
    "                if np.mean(rew_buffer) > MAX_REWARD_REACH:\n",
    "                    if np.mean(rew_buffer) > Reward_max : \n",
    "                        print(f'current_Reward_max {Reward_max}')\n",
    "                        print(f'Trained_Reward_max {np.mean(rew_buffer)}')\n",
    "                        Reward_max = np.mean(rew_buffer)\n",
    "    \n",
    "                        if np.mean(rew_buffer) >= MAX_REWARD_REACH:\n",
    "    \n",
    "                            checkpoint = {\n",
    "                                \"ENV_Name\":ENV_Name,\n",
    "                                \"state_dict\": online_net.state_dict(),\n",
    "                                \"GAMMA\":GAMMA,\n",
    "                                \"BATCH_SIZE\":BATCH_SIZE,\n",
    "                                \"BUFFER_SIZE\":BUFFER_SIZE,\n",
    "                                \"MIN_REPLAY_SIZE\":MIN_REPLAY_SIZE,\n",
    "                                \"EPSILON_START\":EPSILON_START,\n",
    "                                \"EPSILON_END\":EPSILON_END,\n",
    "                                \"EPSILON_DECAY\":EPSILON_DECAY,\n",
    "                                \"MAX_REWARD_REACH\" :MAX_REWARD_REACH,\n",
    "                                \"TARGET_UPDATE_FREQ\" :TARGET_UPDATE_FREQ,\n",
    "                                \"LR\" :LR}\n",
    "                            if Mean_Flag:\n",
    "                                model_name=f'iteration_{iteration_no}_{ENV_Name}_Qmean_best'\n",
    "                            else:\n",
    "                                model_name=f'iteration_{iteration_no}_{ENV_Name}_Qmax_best'\n",
    "                            save_checkpoint(checkpoint,model_name)\n",
    "\n",
    "#                 break\n",
    "\n",
    "        transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "        obses = []\n",
    "        actions = []\n",
    "        rews = []\n",
    "        dones = []\n",
    "        new_obses = []\n",
    "\n",
    "        for t in transitions:\n",
    "            obses.append(t[0])\n",
    "            actions.append(t[1])\n",
    "            rews.append(t[2])\n",
    "            dones.append(t[3])\n",
    "            new_obses.append(t[4])\n",
    "\n",
    "\n",
    "        obses = np.array(obses)\n",
    "        actions = np.array(actions)\n",
    "        rews = np.array(rews)\n",
    "        dones = np.array(dones)\n",
    "        new_obses = np.array(new_obses)\n",
    "\n",
    "        obses_t = torch.as_tensor(obses, dtype=torch.float32).to(device)\n",
    "        actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1).to(device)\n",
    "        rews_t = torch.as_tensor(rews, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "        dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "        new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Compute Targets\n",
    "        # targets = r + gamma * target q vals * (1 - dones)\n",
    "        target_q_values = target_net(new_obses_t)\n",
    "        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "        targets = rews_t + GAMMA * (1 - dones_t) * max_target_q_values\n",
    "\n",
    "        # Compute Loss\n",
    "        q_values = online_net(obses_t)\n",
    "        action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "\n",
    "        # loss = nn.functional.smooth_l1_loss(action_q_values, targets)\n",
    "        loss = nn.functional.mse_loss(action_q_values, targets)\n",
    "\n",
    "        # Gradient Descent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update Target Net\n",
    "        if step % TARGET_UPDATE_FREQ == 0:\n",
    "            target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "        # Logging\n",
    "        if step % 10000 == 0:\n",
    "            if step !=0:\n",
    "                print(f'epsilon -- > {epsilon}')\n",
    "                print('Step:', step)\n",
    "                print('Avg Rew:', np.mean(rew_buffer))\n",
    "\n",
    "            \n",
    "        if step % 1000 == 0:\n",
    "            if step !=0:\n",
    "                Average_reward.append(np.mean(rew_buffer))\n",
    "    \n",
    "    checkpoint = {\n",
    "        \"ENV_Name\":ENV_Name,\n",
    "        \"state_dict\": online_net.state_dict(),\n",
    "        \"GAMMA\":GAMMA,\n",
    "        \"BATCH_SIZE\":BATCH_SIZE,\n",
    "        \"BUFFER_SIZE\":BUFFER_SIZE,\n",
    "        \"MIN_REPLAY_SIZE\":MIN_REPLAY_SIZE,\n",
    "        \"EPSILON_START\":EPSILON_START,\n",
    "        \"EPSILON_END\":EPSILON_END,\n",
    "        \"EPSILON_DECAY\":EPSILON_DECAY,\n",
    "        \"MAX_REWARD_REACH\" :MAX_REWARD_REACH,\n",
    "        \"TARGET_UPDATE_FREQ\" :TARGET_UPDATE_FREQ,\n",
    "        \"LR\" :LR}\n",
    "    if Mean_Flag:\n",
    "        model_name=f'iteration_{iteration_no}_{ENV_Name}_Qmean_last'\n",
    "    else:\n",
    "        model_name=f'iteration_{iteration_no}_{ENV_Name}_Qmax_last'\n",
    "    save_checkpoint(checkpoint,model_name)\n",
    "    \n",
    "\n",
    "    return Average_reward,online_net\n",
    "\n",
    "def save_checkpoint(state,model_name):\n",
    "    filename=f'./All_ckpt/{model_name}_checkpoint__.pth'\n",
    "#     print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "def View_output(online_net):\n",
    "    obs = env.reset()\n",
    "    episode_length = 0\n",
    "    for i in range(100):\n",
    "        print(f'episode_length {episode_length}')\n",
    "        episode_length +=1\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32).to(device)\n",
    "        action = online_net.act(obs)\n",
    "\n",
    "        obs, _, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        if done: \n",
    "            env.reset()\n",
    "            episode_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38aabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  ITERATION   0 \n",
      "\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imaging/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_Reward_max -1000\n",
      "Trained_Reward_max -476.1904761904762\n",
      "epsilon -- > 0.9346666666666666\n",
      "Step: 10000\n",
      "Avg Rew: -476.1904761904762\n",
      "current_Reward_max -1000\n",
      "Trained_Reward_max -486.7560975609756\n",
      "epsilon -- > 0.8693333333333333\n",
      "Step: 20000\n",
      "Avg Rew: -486.7560975609756\n",
      "current_Reward_max -1000\n",
      "Trained_Reward_max -458.26153846153846\n",
      "epsilon -- > 0.804\n",
      "Step: 30000\n",
      "Avg Rew: -458.26153846153846\n",
      "current_Reward_max -1000\n",
      "Trained_Reward_max -421.3404255319149\n",
      "epsilon -- > 0.7386666666666667\n",
      "Step: 40000\n",
      "Avg Rew: -421.3404255319149\n",
      "current_Reward_max -1000\n",
      "Trained_Reward_max -343.64\n",
      "epsilon -- > 0.6733333333333333\n",
      "Step: 50000\n",
      "Avg Rew: -343.64\n",
      "current_Reward_max -1000\n",
      "Trained_Reward_max -249.09\n",
      "epsilon -- > 0.608\n",
      "Step: 60000\n",
      "Avg Rew: -249.09\n",
      "current_Reward_max -1000\n",
      "Trained_Reward_max -194.37\n",
      "epsilon -- > 0.5426666666666666\n",
      "Step: 70000\n",
      "Avg Rew: -194.37\n",
      "current_Reward_max -1000\n",
      "Trained_Reward_max -171.33\n",
      "epsilon -- > 0.4773333333333334\n",
      "Step: 80000\n",
      "Avg Rew: -171.33\n"
     ]
    }
   ],
   "source": [
    "Mean_Flag = True\n",
    "Iterations =5\n",
    "total_reward_list = []\n",
    "for k in range(Iterations):\n",
    "    random_number = np.random.randint(1, 100)\n",
    "    torch.manual_seed(random_number)\n",
    "    env.seed(random_number)\n",
    "    env.reset()\n",
    "    print(f'\\n\\n\\n  ITERATION   {k} \\n\\n ' )\n",
    "    Average_reward,online_net = train(k,Mean_Flag)\n",
    "#     View_output(online_net)\n",
    "    Average_reward = np.array(Average_reward)\n",
    "    total_reward_list.append(Average_reward)\n",
    "    file_name = f'./npy/Q1_itertaion_{k}_mean_{Mean_Flag}_{ENV_Name}_'\n",
    "\n",
    "    np.save(f'{file_name}.npy', Average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df4e68-12d2-4133-8b04-1b833244852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean_Flag = True\n",
    "# mean_Plot_array = []\n",
    "# for k in range(Iterations):\n",
    "#     file_name = f'./npy/Q1_itertaion_{k}_mean_{Mean_Flag}_{ENV_Name}_.npy'\n",
    "#     loaded_array = np.load(file_name)\n",
    "#     mean_Plot_array.append(loaded_array)\n",
    "\n",
    "# mean_scrs = np.mean(mean_Plot_array,axis = 0)\n",
    "\n",
    "# plt.plot(mean_scrs,label='Mean DQN')\n",
    "# plt.legend()\n",
    "# plt.ylabel ('Total Reward')\n",
    "# plt.xlabel('Episodes')\n",
    "# plt.title(f'Reward curve (Average over {Iterations} runs)')\n",
    "# plt.savefig('Mean_DQN_CartPole-v0.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f272b0a-ea9e-4183-8e85-0c9d5e7da8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean_Flag = False\n",
    "# max_Plot_array = []\n",
    "# for k in range(Iterations):\n",
    "#     file_name = f'./npy/Q1_itertaion_{k}_mean_{Mean_Flag}_{ENV_Name}_.npy'\n",
    "#     loaded_array = np.load(file_name)\n",
    "#     max_Plot_array.append(loaded_array)\n",
    "\n",
    "\n",
    "# max_scrs = np.mean(max_Plot_array,axis = 0)\n",
    "\n",
    "# plt.plot(max_scrs,label='Max DQN')\n",
    "# plt.legend()\n",
    "# plt.ylabel ('Total Reward')\n",
    "# plt.xlabel('Episodes')\n",
    "# plt.title(f'Reward curve (Average over {Iterations} runs)')\n",
    "# plt.savefig('Max_DQN_CartPole-v0.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7390058-66b2-4117-a60c-30d756ac5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean_Flag = True\n",
    "# mean_Plot_array = []\n",
    "# for k in range(Iterations):\n",
    "#     file_name = f'./npy/Q1_itertaion_{k}_mean_{Mean_Flag}_{ENV_Name}_.npy'\n",
    "#     loaded_array = np.load(file_name)\n",
    "#     mean_Plot_array.append(loaded_array)\n",
    "\n",
    "# Mean_Flag = False\n",
    "# max_Plot_array = []\n",
    "# for k in range(Iterations):\n",
    "#     file_name = f'./npy/Q1_itertaion_{k}_mean_{Mean_Flag}_{ENV_Name}_.npy'\n",
    "#     loaded_array = np.load(file_name)\n",
    "#     max_Plot_array.append(loaded_array)\n",
    "\n",
    "# mean_scrs = np.mean(mean_Plot_array,axis = 0)\n",
    "# max_scrs = np.mean(max_Plot_array,axis = 0)\n",
    "\n",
    "# plt.plot(mean_scrs,label='Mean DQN')\n",
    "# plt.plot(max_scrs,label='Max DQN')\n",
    "# plt.legend()\n",
    "# plt.ylabel ('Total Reward')\n",
    "# plt.xlabel('Episodes')\n",
    "# plt.title(f'Reward curve (Average over {Iterations} runs)')\n",
    "# plt.savefig('DQN_CartPole-v0.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d8e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# obs = env.reset()\n",
    "# episode_length = 0\n",
    "# for i in range(5000):\n",
    "#     print(f'episode_length {episode_length}')\n",
    "#     episode_length +=1\n",
    "#     obs = torch.as_tensor(obs, dtype=torch.float32).to(device)\n",
    "#     action = online_net.act(obs)\n",
    "\n",
    "#     obs, _, done, _ = env.step(action)\n",
    "#     env.render()\n",
    "#     if done: \n",
    "#         env.reset()\n",
    "#         episode_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8264c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
