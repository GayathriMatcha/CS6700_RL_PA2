{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imaging/anaconda3/lib/python3.11/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/imaging/anaconda3/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "MAX_REWARD_REACH = 175\n",
    "BATCH_SIZE=256\n",
    "GAMMA = 0.99 #discount factor for future utilities\n",
    "env = gym.make('CartPole-v1') #Make environment\n",
    "NUM_EPISODES = 3000 #number of episodes to run\n",
    "MAX_STEPS = 10000 #max steps per episode\n",
    "SOLVED_SCORE = 195 #score agent needs for environment to be solved\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" #device to run model on \n",
    "Model_Name = \"Reinforce_Base_line\"\n",
    "LR = 1e-3\n",
    "\n",
    "ENV_Name='CartPole-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imaging/anaconda3/lib/python3.11/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/imaging/anaconda3/lib/python3.11/site-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(ENV_Name)\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a neural network to learn our policy parameters\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    #Takes in observations and outputs actions\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(observation_space, 128)\n",
    "        self.output_layer = nn.Linear(128, action_space)\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, x):\n",
    "        #input states\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        #relu activation\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #actions\n",
    "        actions = self.output_layer(x)\n",
    "        \n",
    "        #get softmax for a probability distribution\n",
    "        action_probs = F.softmax(actions, dim=1)\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a neural network to learn state value\n",
    "class StateValueNetwork(nn.Module):\n",
    "    \n",
    "    #Takes in state\n",
    "    def __init__(self, observation_space):\n",
    "        super(StateValueNetwork, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(observation_space, 128)\n",
    "        self.output_layer = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #input layer\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        #activiation relu\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        #get state value\n",
    "        state_value = self.output_layer(x)\n",
    "        \n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(network, state):\n",
    "    ''' Selects an action given current state\n",
    "    Args:\n",
    "    - network (Torch NN): network to process state\n",
    "    - state (Array): Array of action space in an environment\n",
    "    \n",
    "    Return:\n",
    "    - (int): action that is selected\n",
    "    - (float): log probability of selecting that action given state and network\n",
    "    '''\n",
    "    \n",
    "    #convert state to float tensor, add 1 dimension, allocate tensor on device\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    #use network to predict action probabilities\n",
    "    action_probs = network(state)\n",
    "    state = state.detach()\n",
    "    \n",
    "    #sample an action using the probability distribution\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample()\n",
    "    \n",
    "    #return action\n",
    "    return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rewards(rewards):\n",
    "    ''' Converts our rewards history into cumulative discounted rewards\n",
    "    Args:\n",
    "    - rewards (Array): array of rewards \n",
    "    \n",
    "    Returns:\n",
    "    - G (Array): array of cumulative discounted rewards\n",
    "    '''\n",
    "    #Calculate Gt (cumulative discounted rewards)\n",
    "    G = []\n",
    "    \n",
    "    #track cumulative reward\n",
    "    total_r = 0\n",
    "    \n",
    "    #iterate rewards from Gt to G0\n",
    "    for r in reversed(rewards):\n",
    "        \n",
    "        #Base case: G(T) = r(T)\n",
    "        #Recursive: G(t) = r(t) + G(t+1)^DISCOUNT\n",
    "        total_r = r + total_r * GAMMA\n",
    "        \n",
    "        #add to front of G\n",
    "        G.insert(0, total_r)\n",
    "    \n",
    "    #whitening rewards\n",
    "    G = torch.tensor(G).to(DEVICE)\n",
    "    G = (G - G.mean())/G.std()\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(deltas, log_probs, optimizer):\n",
    "    ''' Update policy parameters\n",
    "    Args:\n",
    "    - deltas (Array): difference between predicted stateval and actual stateval (Gt)\n",
    "    - log_probs (Array): trajectory of log probabilities of action taken\n",
    "    - optimizer (Pytorch optimizer): optimizer to update policy network parameters\n",
    "    '''\n",
    "    \n",
    "    #store updates\n",
    "    policy_loss = []\n",
    "    \n",
    "    #calculate loss to be backpropagated\n",
    "    for d, lp in zip(deltas, log_probs):\n",
    "        #add negative sign since we are performing gradient ascent\n",
    "        policy_loss.append(-d * lp)\n",
    "    \n",
    "    #Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    sum(policy_loss).backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_value(G, state_vals, optimizer):\n",
    "    ''' Update state-value network parameters\n",
    "    Args:\n",
    "    - G (Array): trajectory of cumulative discounted rewards \n",
    "    - state_vals (Array): trajectory of predicted state-value at each step\n",
    "    - optimizer (Pytorch optimizer): optimizer to update state-value network parameters\n",
    "    '''\n",
    "    \n",
    "    #calculate MSE loss\n",
    "    val_loss = F.mse_loss(state_vals, G)\n",
    "        \n",
    "    #Backpropagate\n",
    "    optimizer.zero_grad()\n",
    "    val_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iteration_no):\n",
    "    ## rest env ###\n",
    "    random_number = np.random.randint(1, 100)\n",
    "    torch.manual_seed(random_number)\n",
    "    env.seed(random_number)\n",
    "    env.reset()\n",
    "    Reward_max = 0\n",
    "\n",
    "    #Init network\n",
    "    policy_network = PolicyNetwork(env.observation_space.shape[0], env.action_space.n).to(DEVICE)\n",
    "    stateval_network = StateValueNetwork(env.observation_space.shape[0]).to(DEVICE)\n",
    "\n",
    "\n",
    "    #Init optimizer\n",
    "    policy_optimizer = optim.Adam(policy_network.parameters(), lr=LR)\n",
    "    stateval_optimizer = optim.Adam(stateval_network.parameters(), lr=LR)\n",
    "\n",
    "    scores = []\n",
    "    Average_reward = []\n",
    "    #recent 100 scores\n",
    "    recent_scores = deque(maxlen=MAX_STEPS)\n",
    "\n",
    "    #iterate through episodes\n",
    "    for episode in tqdm_notebook(range(NUM_EPISODES)):\n",
    "\n",
    "        #reset environment, initiable variables\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "        score = 0\n",
    "        rew_buffer = deque(maxlen=MAX_STEPS)\n",
    "\n",
    "        #generate episode\n",
    "        for step in range(MAX_STEPS):\n",
    "            #env.render()\n",
    "\n",
    "            #select action\n",
    "            action, lp = select_action(policy_network, state)\n",
    "\n",
    "            #execute action\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            #track episode score\n",
    "            score += reward\n",
    "\n",
    "            #store into trajectory\n",
    "            trajectory.append([state, action, reward, lp])\n",
    "\n",
    "            #end episode\n",
    "            if done:\n",
    "                rew_buffer.append(score)\n",
    "                break\n",
    "\n",
    "            #move into new state\n",
    "            state = new_state\n",
    "\n",
    "        #append score\n",
    "        scores.append(score)\n",
    "        recent_scores.append(score)\n",
    "\n",
    "        # # If we solved it lets just watch it play, put in last\n",
    "        if episode % 100 == 0:\n",
    "            if np.mean(rew_buffer) > MAX_REWARD_REACH:\n",
    "                if np.mean(rew_buffer)> Reward_max : \n",
    "                    print(f'current_Reward_max {Reward_max}')\n",
    "                    print(f'Trained_Reward_max {np.mean(rew_buffer)}')\n",
    "                    Reward_max = np.mean(rew_buffer)\n",
    "\n",
    "                    if np.mean(rew_buffer) >= MAX_REWARD_REACH:\n",
    "\n",
    "                        checkpoint = {\n",
    "                            \"ENV_Name\":ENV_Name,\n",
    "                            \"state_dict\": policy_network.state_dict(),\n",
    "                            \"GAMMA\":GAMMA,\n",
    "                            \"BATCH_SIZE\":BATCH_SIZE,\n",
    "                            \"MAX_REWARD_REACH\" :MAX_REWARD_REACH,\n",
    "                            \"LR\" :LR}\n",
    "                        model_name=f'iteration_{iteration_no}_{ENV_Name}_{Model_Name}_best'\n",
    "                        save_checkpoint(checkpoint,model_name)\n",
    "\n",
    "\n",
    "        #get items from trajectory\n",
    "        states = [step[0] for step in trajectory]\n",
    "        actions = [step[1] for step in trajectory]\n",
    "        rewards = [step[2] for step in trajectory]\n",
    "        lps = [step[3] for step in trajectory]\n",
    "\n",
    "        #get discounted rewards\n",
    "        G = process_rewards(rewards)\n",
    "        #G = torch.tensor(G).to(DEVICE)\n",
    "\n",
    "        #calculate state values and train statevalue network\n",
    "        state_vals = []\n",
    "        for state in states:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "            state_vals.append(stateval_network(state))\n",
    "\n",
    "        state_vals = torch.stack(state_vals).squeeze()\n",
    "\n",
    "        train_value(G, state_vals, stateval_optimizer)\n",
    "\n",
    "        #calculate deltas and train policy network\n",
    "        deltas = [gt - val for gt, val in zip(G, state_vals)]\n",
    "        deltas = torch.tensor(deltas).to(DEVICE)\n",
    "\n",
    "        train_policy(deltas, lps, policy_optimizer)\n",
    "\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            if episode !=0:\n",
    "#                 print(f'length--> {len(rew_buffer)}')\n",
    "#                 print(f'average reward {rew_buffer[0]}')\n",
    "                Average_reward.append(rew_buffer[0])\n",
    "    \n",
    "        if episode % 250 == 0:\n",
    "            if episode !=0:\n",
    "#                 print(f'length--> {len(rew_buffer)}')\n",
    "                print(f'average reward {rew_buffer[0]}')\n",
    "    \n",
    "    checkpoint = {\n",
    "        \"ENV_Name\":ENV_Name,\n",
    "        \"state_dict\": policy_network.state_dict(),\n",
    "        \"GAMMA\":GAMMA,\n",
    "        \"BATCH_SIZE\":BATCH_SIZE,\n",
    "        \"MAX_REWARD_REACH\" :MAX_REWARD_REACH,\n",
    "        \"LR\" :LR}\n",
    "    model_name=f'iteration_{iteration_no}_{ENV_Name}_{Model_Name}_last'\n",
    "    save_checkpoint(checkpoint,model_name)\n",
    "    \n",
    "    return Average_reward,policy_network\n",
    "    \n",
    "def save_checkpoint(state,model_name):\n",
    "    filename=f'./All_ckpt/{model_name}_checkpoint__.pth'\n",
    "#     print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  ITERATION   0 \n",
      "\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_265920/114309305.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for episode in tqdm_notebook(range(NUM_EPISODES)):\n",
      "/home/imaging/anaconda3/lib/python3.11/site-packages/ipywidgets/widgets/widget.py:438: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n",
      "  self.comm = Comm(**args)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5507dbf218654237b4cead08b1a5e3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imaging/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward 124.0\n",
      "current_Reward_max 0\n",
      "Trained_Reward_max 200.0\n",
      "average reward 200.0\n",
      "average reward 200.0\n",
      "average reward 200.0\n",
      "average reward 200.0\n",
      "average reward 183.0\n",
      "average reward 200.0\n",
      "average reward 200.0\n",
      "average reward 165.0\n",
      "average reward 200.0\n"
     ]
    }
   ],
   "source": [
    "Iterations =5\n",
    "total_reward_list = []\n",
    "for k in range(Iterations):\n",
    "    print(f'\\n\\n\\n  ITERATION   {k} \\n\\n ' )\n",
    "    Average_reward,policy_network = train(k)\n",
    "    Average_reward = np.array(Average_reward)\n",
    "    total_reward_list.append(Average_reward)\n",
    "    file_name = f'./npy/Q2_itertaion_{k}_{Model_Name}_{ENV_Name}_'\n",
    "    np.save(f'{file_name}.npy', Average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_Flag = True\n",
    "mean_Plot_array = []\n",
    "for k in range(Iterations):\n",
    "    file_name = f'./npy/Q2_itertaion_{k}_{Model_Name}_{ENV_Name}_'\n",
    "    loaded_array = np.load(f'{file_name}.npy')\n",
    "    mean_Plot_array.append(loaded_array)\n",
    "\n",
    "mean_scrs = np.mean(mean_Plot_array,axis = 0)\n",
    "\n",
    "plt.plot(mean_scrs,label=f'{ENV_Name}')\n",
    "plt.legend()\n",
    "plt.ylabel ('Total Reward')\n",
    "plt.xlabel('Episodes')\n",
    "plt.title(f'{ENV_Name} with {Model_Name}(Average over {Iterations} runs)')\n",
    "plt.savefig(f'{ENV_Name} with {Model_Name}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     episode_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     obs = torch.as_tensor(obs, dtype=torch.float32).to(device)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     action,_ \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     policy_network.act(obs)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     obs, _, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(network, state)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#sample an action using the probability distribution\u001b[39;00m\n\u001b[1;32m     20\u001b[0m m \u001b[38;5;241m=\u001b[39m Categorical(action_probs)\n\u001b[0;32m---> 21\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#return action\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem(), m\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/distributions/categorical.py:118\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    117\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 118\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "episode_length = 0\n",
    "for i in range(5000):\n",
    "    print(f'episode_length {episode_length}')\n",
    "    episode_length +=1\n",
    "#     obs = torch.as_tensor(obs, dtype=torch.float32).to(device)\n",
    "    action,_ = select_action(policy_network, obs)\n",
    "#     policy_network.act(obs)\n",
    "\n",
    "    obs, _, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done: \n",
    "        env.reset()\n",
    "        episode_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
